{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xro4zR6kVxCB",
        "outputId": "b5cdd26b-850a-4e00-e8a4-df7bbbd98e22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column names in the dataset: Index(['Serial_Number', 'Error_Type', 'Ungrammatical_Statement',\n",
            "       'Standard_English'],\n",
            "      dtype='object')\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found:  {'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None}\n",
            "Random Forest Model evaluation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.22      0.33         9\n",
            "           1       0.67      0.36      0.47        11\n",
            "           2       0.00      0.00      0.00        11\n",
            "           3       0.08      0.09      0.08        22\n",
            "           4       0.00      0.00      0.00         6\n",
            "           5       0.19      0.33      0.24         9\n",
            "           6       0.43      0.30      0.35        10\n",
            "           7       0.00      0.00      0.00         7\n",
            "           8       0.00      0.00      0.00        12\n",
            "           9       0.83      0.42      0.56        12\n",
            "          10       1.00      0.08      0.15        12\n",
            "          11       0.00      0.00      0.00        15\n",
            "          12       0.00      0.00      0.00         9\n",
            "          13       0.78      0.78      0.78         9\n",
            "          14       0.40      0.20      0.27        10\n",
            "          15       0.77      1.00      0.87        10\n",
            "          16       0.38      0.75      0.50         4\n",
            "          17       1.00      0.08      0.15        12\n",
            "          18       0.44      0.47      0.45        15\n",
            "          19       0.25      0.25      0.25         4\n",
            "          20       0.00      0.00      0.00        10\n",
            "          21       0.27      0.48      0.35        25\n",
            "          22       0.00      0.00      0.00        10\n",
            "          23       0.08      0.11      0.09         9\n",
            "          24       0.71      1.00      0.83         5\n",
            "          25       0.00      0.00      0.00         1\n",
            "          26       0.33      0.11      0.17         9\n",
            "          27       0.00      0.00      0.00         5\n",
            "          28       0.33      0.11      0.17         9\n",
            "          29       0.23      0.29      0.26        24\n",
            "          30       0.33      0.21      0.26        14\n",
            "          31       0.08      0.16      0.10        19\n",
            "          32       0.21      0.50      0.30        18\n",
            "          33       0.00      0.00      0.00         9\n",
            "          34       0.15      0.47      0.23        19\n",
            "          35       0.00      0.00      0.00         9\n",
            "\n",
            "    accuracy                           0.25       404\n",
            "   macro avg       0.29      0.24      0.23       404\n",
            "weighted avg       0.29      0.25      0.23       404\n",
            "\n",
            "Accuracy: 0.2524752475247525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a sentence for error detection and correction (or type 'exit' to quit): i go to the store everyday.\n",
            "Detected Error Type: Verb Tense Errors\n",
            "Corrected Sentence: I go to the store everyday.\n",
            "Enter a sentence for error detection and correction (or type 'exit' to quit): i goes to the store everyday.\n",
            "Detected Error Type: Verb Tense Errors\n",
            "Corrected Sentence: I go to the store every day.\n",
            "Enter a sentence for error detection and correction (or type 'exit' to quit): i wanting a cofee.\n",
            "Detected Error Type: Spelling Mistakes\n",
            "Corrected Sentence: I want a room.\n",
            "Enter a sentence for error detection and correction (or type 'exit' to quit): i wanting a coffee.\n",
            "Detected Error Type: Spelling Mistakes\n",
            "Corrected Sentence: I want a coffee.\n",
            "Enter a sentence for error detection and correction (or type 'exit' to quit): i think the code is done.\n",
            "Detected Error Type: Spelling Mistakes\n",
            "Corrected Sentence: I think the code is done.\n",
            "Enter a sentence for error detection and correction (or type 'exit' to quit): I think the code is done.\n",
            "Detected Error Type: Spelling Mistakes\n",
            "Corrected Sentence: I think the code is done.\n",
            "Enter a sentence for error detection and correction (or type 'exit' to quit): I was a child when i was six he was five i was six i hit the ground bang bang my baby shot be down bang bang.\n",
            "Detected Error Type: Spelling Mistakes\n",
            "Corrected Sentence: I was a child when I was six he was five i was six i hit the ground bang bang my baby shot down bang bang.\n",
            "Enter a sentence for error detection and correction (or type 'exit' to quit): i is go to the ground in 5 pm then i play football and we i score a goal.\n",
            "Detected Error Type: Punctuation Errors\n",
            "Corrected Sentence: I go to the ground at 5 pm and then I play football and we score a goal.\n",
            "Enter a sentence for error detection and correction (or type 'exit' to quit): i is go to the ground in 5 pm then i play football.\n",
            "Detected Error Type: Sentence Structure Errors\n",
            "Corrected Sentence: I go to the ground at 5 pm and then I play football.\n",
            "Enter a sentence for error detection and correction (or type 'exit' to quit): i wanting a rest.\n",
            "Detected Error Type: Spelling Mistakes\n",
            "Corrected Sentence: I want a rest.\n",
            "Enter a sentence for error detection and correction (or type 'exit' to quit): I want a rest.\n",
            "Detected Error Type: Sentence Structure Errors\n",
            "Corrected Sentence: I want a rest.\n",
            "Enter a sentence for error detection and correction (or type 'exit' to quit): I need a rest.\n",
            "Detected Error Type: Sentence Structure Errors\n",
            "Corrected Sentence: I need a rest.\n",
            "Enter a sentence for error detection and correction (or type 'exit' to quit): I want a coffe.\n",
            "Detected Error Type: Conjunction Misuse\n",
            "Corrected Sentence: I want a coffee.\n",
            "Enter a sentence for error detection and correction (or type 'exit' to quit): me wanting a flower.\n",
            "Detected Error Type: Spelling Mistakes\n",
            "Corrected Sentence: Me wanting a flower.\n",
            "Enter a sentence for error detection and correction (or type 'exit' to quit): i wanting a flower. \n",
            "Detected Error Type: Spelling Mistakes\n",
            "Corrected Sentence: I want a flower.\n",
            "Enter a sentence for error detection and correction (or type 'exit' to quit): ghar phuk k maal nahi chalta.\n",
            "Detected Error Type: Spelling Mistakes\n",
            "Corrected Sentence: Ghar phuk k maal nahi chalta.\n",
            "Enter a sentence for error detection and correction (or type 'exit' to quit): i store to the goes everyday.\n",
            "Detected Error Type: Verb Tense Errors\n",
            "Corrected Sentence: I store to the goes everyday.\n",
            "Enter a sentence for error detection and correction (or type 'exit' to quit): exit\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Uncomment these lines if you haven't downloaded the nltk data before\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load sample data\n",
        "data = pd.read_csv(\"/content/Grammar Correction.csv\")  # Replace with actual dataset path\n",
        "\n",
        "# Print column names to verify\n",
        "print(\"Column names in the dataset:\", data.columns)\n",
        "\n",
        "# Check if the necessary columns are present\n",
        "if 'Ungrammatical_Statement' not in data.columns or 'Error_Type' not in data.columns:\n",
        "    raise KeyError(\"The dataset must contain 'Ungrammatical_Statement' and 'Error_Type' columns\")\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(Ungrammatical_Statement):\n",
        "    # Lowercase conversion\n",
        "    # Ungrammatical_Statement = Ungrammatical_Statement.lower()\n",
        "    # Remove special characters\n",
        "    Ungrammatical_Statement = re.sub(r'\\W', ' ', Ungrammatical_Statement)\n",
        "    # Tokenization\n",
        "    words = word_tokenize(Ungrammatical_Statement)\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply preprocessing\n",
        "data['cleaned_text'] = data['Ungrammatical_Statement'].apply(preprocess_text)\n",
        "\n",
        "# Label encoding (assuming binary classification for errors)\n",
        "label_encoder = LabelEncoder()\n",
        "data['label'] = label_encoder.fit_transform(data['Error_Type'])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['cleaned_text'], data['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Define the parameter grid for Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_grid, n_iter=10, cv=3, verbose=2, n_jobs=-1)\n",
        "\n",
        "# Fit RandomizedSearchCV to the training data\n",
        "random_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best parameters found: \", random_search.best_params_)\n",
        "\n",
        "# Get the best model\n",
        "best_rf_model = random_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the test data\n",
        "y_pred_rf = best_rf_model.predict(X_test_tfidf)\n",
        "print(\"Random Forest Model evaluation:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf)}\")\n",
        "\n",
        "# Load the pre-trained T5 model and tokenizer fine-tuned for grammar correction\n",
        "model_name = 'vennify/t5-base-grammar-correction'  # Known model for grammar correction\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Function to correct grammatical errors using the T5 model\n",
        "def correct_grammar(text, model, tokenizer):\n",
        "    input_text = \"gec: \" + text\n",
        "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(inputs, max_length=512, num_beams=4, early_stopping=True)\n",
        "    corrected_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return corrected_text\n",
        "\n",
        "# Function to detect errors and correct the sentence\n",
        "def detect_and_correct_errors(text, rf_model, vectorizer, t5_model, t5_tokenizer, label_encoder):\n",
        "    # Preprocess the text for error detection\n",
        "    cleaned_text = preprocess_text(text)\n",
        "    text_tfidf = vectorizer.transform([cleaned_text])\n",
        "    # Detect the error type\n",
        "    predicted_label = rf_model.predict(text_tfidf)\n",
        "    # Check if the sentence has no errors based on some threshold\n",
        "    if predicted_label == 0:  # Assuming 0 is the label for \"no error\"\n",
        "        return \"No error in the sentence\", text\n",
        "    error_type = label_encoder.inverse_transform(predicted_label)[0]\n",
        "    # Correct the sentence using the T5 model\n",
        "    corrected_sentence = correct_grammar(text, t5_model, t5_tokenizer)\n",
        "    return error_type, corrected_sentence\n",
        "\n",
        "# Take user input for testing the model\n",
        "while True:\n",
        "    user_input = input(\"Enter a sentence for error detection and correction (or type 'exit' to quit): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    detected_error_type, corrected_sentence = detect_and_correct_errors(user_input, best_rf_model, tfidf_vectorizer, t5_model, t5_tokenizer, label_encoder)\n",
        "    print(f\"Detected Error Type: {detected_error_type}\")\n",
        "    print(f\"Corrected Sentence: {corrected_sentence}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Uncomment these lines if you haven't downloaded the nltk data before\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load sample data\n",
        "data = pd.read_csv(\"/content/Grammar Correction.csv\")  # Replace with actual dataset path\n",
        "\n",
        "# Print column names to verify\n",
        "print(\"Column names in the dataset:\", data.columns)\n",
        "\n",
        "# Check if the necessary columns are present\n",
        "if 'Ungrammatical_Statement' not in data.columns or 'Error_Type' not in data.columns:\n",
        "    raise KeyError(\"The dataset must contain 'Ungrammatical_Statement' and 'Error_Type' columns\")\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(Ungrammatical_Statement):\n",
        "    # Lowercase conversion\n",
        "    # Ungrammatical_Statement = Ungrammatical_Statement.lower()\n",
        "    # Remove special characters\n",
        "    Ungrammatical_Statement = re.sub(r'\\W', ' ', Ungrammatical_Statement)\n",
        "    # Tokenization\n",
        "    words = word_tokenize(Ungrammatical_Statement)\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply preprocessing\n",
        "data['cleaned_text'] = data['Ungrammatical_Statement'].apply(preprocess_text)\n",
        "\n",
        "# Label encoding (assuming binary classification for errors)\n",
        "label_encoder = LabelEncoder()\n",
        "data['label'] = label_encoder.fit_transform(data['Error_Type'])\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(data['cleaned_text'])\n",
        "\n",
        "# Apply K-means clustering\n",
        "num_clusters = 10  # Set the number of clusters\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "data['cluster'] = kmeans.fit_predict(tfidf_matrix)\n",
        "\n",
        "# Add cluster labels as features\n",
        "X = np.hstack((tfidf_matrix.toarray(), data['cluster'].values.reshape(-1, 1)))\n",
        "y = data['label']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_grid, n_iter=10, cv=3, verbose=2, n_jobs=-1)\n",
        "\n",
        "# Fit RandomizedSearchCV to the training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best parameters found: \", random_search.best_params_)\n",
        "\n",
        "# Get the best model\n",
        "best_rf_model = random_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the test data\n",
        "y_pred_rf = best_rf_model.predict(X_test)\n",
        "print(\"Random Forest Model evaluation:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf)}\")\n",
        "\n",
        "# Load the pre-trained T5 model and tokenizer fine-tuned for grammar correction\n",
        "model_name = 'vennify/t5-base-grammar-correction'  # Known model for grammar correction\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Function to correct grammatical errors using the T5 model\n",
        "def correct_grammar(text, model, tokenizer):\n",
        "    input_text = \"gec: \" + text\n",
        "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(inputs, max_length=512, num_beams=4, early_stopping=True)\n",
        "    corrected_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return corrected_text\n",
        "\n",
        "# Function to detect errors and correct the sentence\n",
        "def detect_and_correct_errors(text, rf_model, vectorizer, kmeans, t5_model, t5_tokenizer, label_encoder):\n",
        "    # Preprocess the text for error detection\n",
        "    cleaned_text = preprocess_text(text)\n",
        "    text_tfidf = vectorizer.transform([cleaned_text])\n",
        "    text_cluster = kmeans.predict(text_tfidf)[0]\n",
        "    text_features = np.hstack((text_tfidf.toarray(), np.array([[text_cluster]])))\n",
        "    # Detect the error type\n",
        "    predicted_label = rf_model.predict(text_features)\n",
        "    # Check if the sentence has no errors based on some threshold\n",
        "    if predicted_label == 0:  # Assuming 0 is the label for \"no error\"\n",
        "        return \"No error in the sentence\", text\n",
        "    error_type = label_encoder.inverse_transform(predicted_label)[0]\n",
        "    # Correct the sentence using the T5 model\n",
        "    corrected_sentence = correct_grammar(text, t5_model, t5_tokenizer)\n",
        "    return error_type, corrected_sentence\n",
        "\n",
        "# Take user input for testing the model\n",
        "while True:\n",
        "    user_input = input(\"Enter a sentence for error detection and correction (or type 'exit' to quit): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    detected_error_type, corrected_sentence = detect_and_correct_errors(user_input, best_rf_model, tfidf_vectorizer, kmeans, t5_model, t5_tokenizer, label_encoder)\n",
        "    print(f\"Detected Error Type: {detected_error_type}\")\n",
        "    print(f\"Corrected Sentence: {corrected_sentence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AX3JDBpZf_EJ",
        "outputId": "703771d8-350a-4a93-eacc-8dfc43144133"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column names in the dataset: Index(['Serial_Number', 'Error_Type', 'Ungrammatical_Statement',\n",
            "       'Standard_English'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found:  {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': None}\n",
            "Random Forest Model evaluation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.11      0.20         9\n",
            "           1       0.80      0.36      0.50        11\n",
            "           2       0.00      0.00      0.00        11\n",
            "           3       0.08      0.09      0.09        22\n",
            "           4       0.14      0.67      0.24         6\n",
            "           5       0.25      0.44      0.32         9\n",
            "           6       0.50      0.30      0.37        10\n",
            "           7       0.50      0.14      0.22         7\n",
            "           8       0.20      0.08      0.12        12\n",
            "           9       0.75      0.50      0.60        12\n",
            "          10       1.00      0.17      0.29        12\n",
            "          11       0.50      0.07      0.12        15\n",
            "          12       0.00      0.00      0.00         9\n",
            "          13       0.86      0.67      0.75         9\n",
            "          14       0.25      0.10      0.14        10\n",
            "          15       0.77      1.00      0.87        10\n",
            "          16       0.30      0.75      0.43         4\n",
            "          17       1.00      0.17      0.29        12\n",
            "          18       0.50      0.40      0.44        15\n",
            "          19       0.00      0.00      0.00         4\n",
            "          20       0.00      0.00      0.00        10\n",
            "          21       0.28      0.44      0.34        25\n",
            "          22       0.00      0.00      0.00        10\n",
            "          23       0.08      0.11      0.10         9\n",
            "          24       0.71      1.00      0.83         5\n",
            "          25       0.00      0.00      0.00         1\n",
            "          26       0.60      0.33      0.43         9\n",
            "          27       0.00      0.00      0.00         5\n",
            "          28       0.50      0.11      0.18         9\n",
            "          29       0.32      0.29      0.30        24\n",
            "          30       0.38      0.21      0.27        14\n",
            "          31       0.22      0.68      0.34        19\n",
            "          32       0.16      0.56      0.25        18\n",
            "          33       0.00      0.00      0.00         9\n",
            "          34       0.30      0.37      0.33        19\n",
            "          35       0.00      0.00      0.00         9\n",
            "\n",
            "    accuracy                           0.29       404\n",
            "   macro avg       0.36      0.28      0.26       404\n",
            "weighted avg       0.37      0.29      0.27       404\n",
            "\n",
            "Accuracy: 0.29207920792079206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a sentence for error detection and correction (or type 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import heapq\n",
        "from collections import defaultdict\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Uncomment these lines if you haven't downloaded the nltk data before\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('stopwords')\n",
        "#nltk.download('wordnet')\n",
        "\n",
        "# Load sample data\n",
        "data = pd.read_csv(\"/content/Grammar Correction.csv\")  # Replace with actual dataset path\n",
        "\n",
        "# Print column names to verify\n",
        "print(\"Column names in the dataset:\", data.columns)\n",
        "\n",
        "# Check if the necessary columns are present\n",
        "if 'Ungrammatical_Statement' not in data.columns or 'Error_Type' not in data.columns:\n",
        "    raise KeyError(\"The dataset must contain 'Ungrammatical_Statement' and 'Error_Type' columns\")\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(Ungrammatical_Statement):\n",
        "    # Lowercase conversion\n",
        "    # Ungrammatical_Statement = Ungrammatical_Statement.lower()\n",
        "    # Remove special characters\n",
        "    Ungrammatical_Statement = re.sub(r'\\W', ' ', Ungrammatical_Statement)\n",
        "    # Tokenization\n",
        "    words = word_tokenize(Ungrammatical_Statement)\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply preprocessing\n",
        "data['cleaned_text'] = data['Ungrammatical_Statement'].apply(preprocess_text)\n",
        "\n",
        "# Label encoding (assuming binary classification for errors)\n",
        "label_encoder = LabelEncoder()\n",
        "data['label'] = label_encoder.fit_transform(data['Error_Type'])\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(data['cleaned_text'])\n",
        "\n",
        "# Apply K-means clustering\n",
        "num_clusters = 10  # Set the number of clusters\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "data['cluster'] = kmeans.fit_predict(tfidf_matrix)\n",
        "\n",
        "# Add cluster labels as features\n",
        "X = np.hstack((tfidf_matrix.toarray(), data['cluster'].values.reshape(-1, 1)))\n",
        "y = data['label']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_grid, n_iter=10, cv=3, verbose=2, n_jobs=-1)\n",
        "\n",
        "# Fit RandomizedSearchCV to the training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best parameters found: \", random_search.best_params_)\n",
        "\n",
        "# Get the best model\n",
        "best_rf_model = random_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the test data\n",
        "y_pred_rf = best_rf_model.predict(X_test)\n",
        "print(\"Random Forest Model evaluation:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf)}\")\n",
        "\n",
        "# Load the pre-trained T5 model and tokenizer fine-tuned for grammar correction\n",
        "model_name = 'vennify/t5-base-grammar-correction'  # Known model for grammar correction\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Function to correct grammatical errors using the T5 model\n",
        "def correct_grammar(text, model, tokenizer):\n",
        "    input_text = \"gec: \" + text\n",
        "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(inputs, max_length=512, num_beams=4, early_stopping=True)\n",
        "    corrected_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return corrected_text\n",
        "\n",
        "# Dijkstra's algorithm for finding the shortest path in the graph of sentence transformations\n",
        "def dijkstra(graph, start, end):\n",
        "    queue, visited = [(0, start, [])], set()\n",
        "    heapq.heapify(queue)\n",
        "    while queue:\n",
        "        (cost, node, path) = heapq.heappop(queue)\n",
        "        if node in visited:\n",
        "            continue\n",
        "        path = path + [node]\n",
        "        if node == end:\n",
        "            return cost, path\n",
        "        visited.add(node)\n",
        "        for (next_node, c) in graph[node]:\n",
        "            if next_node not in visited:\n",
        "                heapq.heappush(queue, (cost + c, next_node, path))\n",
        "    return float(\"inf\"), []\n",
        "\n",
        "# Create a graph of possible sentence transformations\n",
        "def create_transformation_graph(sentence, model, tokenizer):\n",
        "    graph = defaultdict(list)\n",
        "    corrected_sentence = correct_grammar(sentence, model, tokenizer)\n",
        "    # For simplicity, assume each transformation has a cost of 1\n",
        "    graph[sentence].append((corrected_sentence, 1))\n",
        "    return graph\n",
        "\n",
        "# Function to detect errors and correct the sentence\n",
        "def detect_and_correct_errors(text, rf_model, vectorizer, kmeans, t5_model, t5_tokenizer, label_encoder):\n",
        "    # Preprocess the text for error detection\n",
        "    cleaned_text = preprocess_text(text)\n",
        "    text_tfidf = vectorizer.transform([cleaned_text])\n",
        "    text_cluster = kmeans.predict(text_tfidf)[0]\n",
        "    text_features = np.hstack((text_tfidf.toarray(), np.array([[text_cluster]])))\n",
        "    # Detect the error type\n",
        "    predicted_label = rf_model.predict(text_features)\n",
        "    # Check if the sentence has no errors based on some threshold\n",
        "    if predicted_label == 0:  # Assuming 0 is the label for \"no error\"\n",
        "        return \"No error in the sentence\", text\n",
        "    error_type = label_encoder.inverse_transform(predicted_label)[0]\n",
        "    # Create a graph of possible sentence transformations\n",
        "    graph = create_transformation_graph(text, t5_model, t5_tokenizer)\n",
        "    # Find the shortest path using Dijkstra's algorithm\n",
        "    cost, path = dijkstra(graph, text, graph[text][0][0])\n",
        "    corrected_sentence = path[-1] if path else text\n",
        "    return error_type, corrected_sentence\n",
        "\n",
        "# Take user input for testing the model\n",
        "while True:\n",
        "    user_input = input(\"Enter a sentence for error detection and correction (or type 'exit' to quit): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    detected_error_type, corrected_sentence = detect_and_correct_errors(user_input, best_rf_model, tfidf_vectorizer, kmeans, t5_model, t5_tokenizer, label_encoder)\n",
        "    print(f\"Detected Error Type: {detected_error_type}\")\n",
        "    print(f\"Corrected Sentence: {corrected_sentence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNiXlzLZhOqZ",
        "outputId": "0372e9e6-2d7b-4f5f-da95-e57fa5365bd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column names in the dataset: Index(['Serial_Number', 'Error_Type', 'Ungrammatical_Statement',\n",
            "       'Standard_English'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found:  {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': None}\n",
            "Random Forest Model evaluation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.11      0.17         9\n",
            "           1       0.67      0.36      0.47        11\n",
            "           2       0.00      0.00      0.00        11\n",
            "           3       0.18      0.18      0.18        22\n",
            "           4       0.16      0.50      0.24         6\n",
            "           5       0.31      0.44      0.36         9\n",
            "           6       0.44      0.40      0.42        10\n",
            "           7       0.25      0.14      0.18         7\n",
            "           8       0.00      0.00      0.00        12\n",
            "           9       0.71      0.42      0.53        12\n",
            "          10       0.20      0.08      0.12        12\n",
            "          11       0.50      0.07      0.12        15\n",
            "          12       0.00      0.00      0.00         9\n",
            "          13       0.71      0.56      0.63         9\n",
            "          14       0.40      0.20      0.27        10\n",
            "          15       0.91      1.00      0.95        10\n",
            "          16       0.67      0.50      0.57         4\n",
            "          17       0.00      0.00      0.00        12\n",
            "          18       0.33      0.47      0.39        15\n",
            "          19       0.00      0.00      0.00         4\n",
            "          20       0.08      0.10      0.09        10\n",
            "          21       0.38      0.40      0.39        25\n",
            "          22       0.00      0.00      0.00        10\n",
            "          23       0.06      0.11      0.08         9\n",
            "          24       0.71      1.00      0.83         5\n",
            "          25       0.00      0.00      0.00         1\n",
            "          26       0.25      0.11      0.15         9\n",
            "          27       0.00      0.00      0.00         5\n",
            "          28       0.20      0.11      0.14         9\n",
            "          29       0.38      0.38      0.38        24\n",
            "          30       0.33      0.14      0.20        14\n",
            "          31       0.20      0.58      0.29        19\n",
            "          32       0.20      0.50      0.28        18\n",
            "          33       1.00      0.11      0.20         9\n",
            "          34       0.35      0.42      0.38        19\n",
            "          35       0.33      0.11      0.17         9\n",
            "\n",
            "    accuracy                           0.28       404\n",
            "   macro avg       0.31      0.26      0.25       404\n",
            "weighted avg       0.32      0.28      0.27       404\n",
            "\n",
            "Accuracy: 0.28217821782178215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a sentence for error detection and correction (or type 'exit' to quit): I is goes to the stores everyday.\n",
            "Detected Error Type: Verb Tense Errors\n",
            "Corrected Sentence: I goes to the stores everyday.\n",
            "Enter a sentence for error detection and correction (or type 'exit' to quit): I is goes to the store everyday.\n",
            "Detected Error Type: Verb Tense Errors\n",
            "Corrected Sentence: I goes to the store everyday.\n",
            "Enter a sentence for error detection and correction (or type 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Uncomment these lines if you haven't downloaded the nltk data before\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('stopwords')\n",
        "#nltk.download('wordnet')\n",
        "\n",
        "# Load sample data\n",
        "data = pd.read_csv(\"/content/Grammar Correction.csv\")  # Replace with actual dataset path\n",
        "\n",
        "# Print column names to verify\n",
        "print(\"Column names in the dataset:\", data.columns)\n",
        "\n",
        "# Check if the necessary columns are present\n",
        "if 'Ungrammatical_Statement' not in data.columns or 'Error_Type' not in data.columns:\n",
        "    raise KeyError(\"The dataset must contain 'Ungrammatical_Statement' and 'Error_Type' columns\")\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(Ungrammatical_Statement):\n",
        "    # Lowercase conversion\n",
        "    # Ungrammatical_Statement = Ungrammatical_Statement.lower()\n",
        "    # Remove special characters\n",
        "    Ungrammatical_Statement = re.sub(r'\\W', ' ', Ungrammatical_Statement)\n",
        "    # Tokenization\n",
        "    words = word_tokenize(Ungrammatical_Statement)\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply preprocessing\n",
        "data['cleaned_text'] = data['Ungrammatical_Statement'].apply(preprocess_text)\n",
        "\n",
        "# Label encoding (assuming binary classification for errors)\n",
        "label_encoder = LabelEncoder()\n",
        "data['label'] = label_encoder.fit_transform(data['Error_Type'])\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(data['cleaned_text'])\n",
        "\n",
        "# Apply K-means clustering\n",
        "num_clusters = 10  # Set the number of clusters\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "data['cluster'] = kmeans.fit_predict(tfidf_matrix)\n",
        "\n",
        "# Add cluster labels as features\n",
        "X = np.hstack((tfidf_matrix.toarray(), data['cluster'].values.reshape(-1, 1)))\n",
        "y = data['label']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_grid, n_iter=10, cv=3, verbose=2, n_jobs=-1)\n",
        "\n",
        "# Fit RandomizedSearchCV to the training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best parameters found: \", random_search.best_params_)\n",
        "\n",
        "# Get the best model\n",
        "best_rf_model = random_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the test data\n",
        "y_pred_rf = best_rf_model.predict(X_test)\n",
        "print(\"Random Forest Model evaluation:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf)}\")\n",
        "\n",
        "# Load the pre-trained T5 model and tokenizer fine-tuned for grammar correction\n",
        "model_name = 'vennify/t5-base-grammar-correction'  # Known model for grammar correction\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Function to correct grammatical errors using the T5 model\n",
        "def correct_grammar(text, model, tokenizer):\n",
        "    input_text = \"gec: \" + text\n",
        "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(inputs, max_length=512, num_beams=4, early_stopping=True)\n",
        "    corrected_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return corrected_text\n",
        "\n",
        "# Function to detect errors and correct the sentence\n",
        "def detect_and_correct_errors(text, rf_model, vectorizer, kmeans, t5_model, t5_tokenizer, label_encoder):\n",
        "    # Preprocess the text for error detection\n",
        "    cleaned_text = preprocess_text(text)\n",
        "    text_tfidf = vectorizer.transform([cleaned_text])\n",
        "    text_cluster = kmeans.predict(text_tfidf)[0]\n",
        "    text_features = np.hstack((text_tfidf.toarray(), np.array([[text_cluster]])))\n",
        "    # Detect the error type\n",
        "    predicted_label = rf_model.predict(text_features)\n",
        "    # Check if the sentence has no errors based on some threshold\n",
        "    if predicted_label == 0:  # Assuming 0 is the label for \"no error\"\n",
        "        return \"No error in the sentence\", text\n",
        "    error_type = label_encoder.inverse_transform(predicted_label)[0]\n",
        "    return error_type, text\n",
        "\n",
        "# Collect all detected errors\n",
        "errors = []\n",
        "# Take user input for testing the model\n",
        "while True:\n",
        "    user_input = input(\"Enter a sentence for error detection and correction (or type 'exit' to quit): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    detected_error_type, original_sentence = detect_and_correct_errors(user_input, best_rf_model, tfidf_vectorizer, kmeans, t5_model, t5_tokenizer, label_encoder)\n",
        "    errors.append((detected_error_type, original_sentence))\n",
        "\n",
        "# Display all detected errors\n",
        "print(\"\\nDetected Errors:\")\n",
        "for i, (error_type, original_sentence) in enumerate(errors, start=1):\n",
        "    print(f\"{i}. Error Type: {error_type}, Sentence: {original_sentence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cR_K03YrhOtf",
        "outputId": "57038f51-3d67-4ee1-aeb5-10a4b2c16773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column names in the dataset: Index(['Serial_Number', 'Error_Type', 'Ungrammatical_Statement',\n",
            "       'Standard_English'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found:  {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': None}\n",
            "Random Forest Model evaluation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.11      0.20         9\n",
            "           1       0.80      0.36      0.50        11\n",
            "           2       0.00      0.00      0.00        11\n",
            "           3       0.14      0.14      0.14        22\n",
            "           4       0.14      0.67      0.24         6\n",
            "           5       0.23      0.33      0.27         9\n",
            "           6       0.50      0.30      0.37        10\n",
            "           7       0.50      0.14      0.22         7\n",
            "           8       0.33      0.08      0.13        12\n",
            "           9       0.75      0.50      0.60        12\n",
            "          10       1.00      0.17      0.29        12\n",
            "          11       0.50      0.07      0.12        15\n",
            "          12       0.00      0.00      0.00         9\n",
            "          13       0.75      0.67      0.71         9\n",
            "          14       0.25      0.10      0.14        10\n",
            "          15       0.77      1.00      0.87        10\n",
            "          16       0.30      0.75      0.43         4\n",
            "          17       1.00      0.17      0.29        12\n",
            "          18       0.50      0.40      0.44        15\n",
            "          19       0.25      0.25      0.25         4\n",
            "          20       0.50      0.10      0.17        10\n",
            "          21       0.24      0.40      0.30        25\n",
            "          22       0.00      0.00      0.00        10\n",
            "          23       0.15      0.22      0.18         9\n",
            "          24       0.71      1.00      0.83         5\n",
            "          25       0.00      0.00      0.00         1\n",
            "          26       0.40      0.22      0.29         9\n",
            "          27       0.00      0.00      0.00         5\n",
            "          28       1.00      0.11      0.20         9\n",
            "          29       0.20      0.21      0.20        24\n",
            "          30       0.33      0.21      0.26        14\n",
            "          31       0.22      0.63      0.32        19\n",
            "          32       0.15      0.50      0.23        18\n",
            "          33       0.00      0.00      0.00         9\n",
            "          34       0.27      0.37      0.31        19\n",
            "          35       0.00      0.00      0.00         9\n",
            "\n",
            "    accuracy                           0.28       404\n",
            "   macro avg       0.39      0.28      0.26       404\n",
            "weighted avg       0.38      0.28      0.26       404\n",
            "\n",
            "Accuracy: 0.28465346534653463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence for error detection and correction (or type 'exit' to quit): exit\n",
            "\n",
            "Detected Errors:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import heapq\n",
        "from collections import defaultdict\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Uncomment these lines if you haven't downloaded the nltk data before\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('stopwords')\n",
        "#nltk.download('wordnet')\n",
        "\n",
        "# Load sample data\n",
        "data = pd.read_csv(\"/content/Grammar Correction.csv\")  # Replace with actual dataset path\n",
        "\n",
        "# Print column names to verify\n",
        "print(\"Column names in the dataset:\", data.columns)\n",
        "\n",
        "# Check if the necessary columns are present\n",
        "if 'Ungrammatical_Statement' not in data.columns or 'Error_Type' not in data.columns:\n",
        "    raise KeyError(\"The dataset must contain 'Ungrammatical_Statement' and 'Error_Type' columns\")\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(Ungrammatical_Statement):\n",
        "    # Remove special characters\n",
        "    Ungrammatical_Statement = re.sub(r'\\W', ' ', Ungrammatical_Statement)\n",
        "    # Tokenization\n",
        "    words = word_tokenize(Ungrammatical_Statement)\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply preprocessing\n",
        "data['cleaned_text'] = data['Ungrammatical_Statement'].apply(preprocess_text)\n",
        "\n",
        "# Label encoding (assuming binary classification for errors)\n",
        "label_encoder = LabelEncoder()\n",
        "data['label'] = label_encoder.fit_transform(data['Error_Type'])\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(data['cleaned_text'])\n",
        "\n",
        "# Apply K-means clustering\n",
        "num_clusters = 10  # Set the number of clusters\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "data['cluster'] = kmeans.fit_predict(tfidf_matrix)\n",
        "\n",
        "# Add cluster labels as features\n",
        "X = np.hstack((tfidf_matrix.toarray(), data['cluster'].values.reshape(-1, 1)))\n",
        "y = data['label']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_grid, n_iter=10, cv=3, verbose=2, n_jobs=-1)\n",
        "\n",
        "# Fit RandomizedSearchCV to the training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best parameters found: \", random_search.best_params_)\n",
        "\n",
        "# Get the best model\n",
        "best_rf_model = random_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the test data\n",
        "y_pred_rf = best_rf_model.predict(X_test)\n",
        "print(\"Random Forest Model evaluation:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf)}\")\n",
        "\n",
        "# Load the pre-trained T5 model and tokenizer fine-tuned for grammar correction\n",
        "model_name = 'vennify/t5-base-grammar-correction'  # Known model for grammar correction\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Dijkstra's algorithm for finding the shortest path in the graph of sentence transformations\n",
        "def dijkstra(graph, start, end):\n",
        "    queue, visited = [(0, start, [])], set()\n",
        "    heapq.heapify(queue)\n",
        "    while queue:\n",
        "        (cost, node, path) = heapq.heappop(queue)\n",
        "        if node in visited:\n",
        "            continue\n",
        "        path = path + [node]\n",
        "        if node == end:\n",
        "            return cost, path\n",
        "        visited.add(node)\n",
        "        for (next_node, c) in graph[node]:\n",
        "            if next_node not in visited:\n",
        "                heapq.heappush(queue, (cost + c, next_node, path))\n",
        "    return float(\"inf\"), []\n",
        "\n",
        "# Create a graph of possible sentence transformations\n",
        "def create_transformation_graph(sentence, model, tokenizer):\n",
        "    graph = defaultdict(list)\n",
        "    corrected_sentence = correct_grammar(sentence, model, tokenizer)\n",
        "    # For simplicity, assume each transformation has a cost of 1\n",
        "    graph[sentence].append((corrected_sentence, 1))\n",
        "    return graph\n",
        "\n",
        "# Function to correct grammatical errors using the T5 model\n",
        "def correct_grammar(text, model, tokenizer):\n",
        "    input_text = \"gec: \" + text\n",
        "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(inputs, max_length=512, num_beams=4, early_stopping=True)\n",
        "    corrected_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return corrected_text\n",
        "\n",
        "# Function to detect errors and correct the sentence\n",
        "def detect_and_correct_errors(text, rf_model, vectorizer, kmeans,\n",
        "                               t5_model, t5_tokenizer, label_encoder):\n",
        "    # Preprocess the text for error detection\n",
        "    cleaned_text = preprocess_text(text)\n",
        "    text_tfidf = vectorizer.transform([cleaned_text])\n",
        "    text_cluster = kmeans.predict(text_tfidf)[0]\n",
        "    text_features = np.hstack((text_tfidf.toarray(), np.array([[text_cluster]])))\n",
        "    # Detect the error types\n",
        "    predicted_labels = rf_model.predict(text_features)\n",
        "    error_types = label_encoder.inverse_transform(predicted_labels)\n",
        "    # Initialize lists to store detected errors and corrected sentences\n",
        "    detected_errors = []\n",
        "    corrected_sentences = []\n",
        "    for error_type in error_types:\n",
        "        # Check if the sentence has no errors based on some threshold\n",
        "        if error_type == \"No_Error\":  # Assuming \"No_Error\" is the label for \"no error\"\n",
        "            detected_errors.append(\"No error in the sentence\")\n",
        "            corrected_sentences.append(text)\n",
        "            continue\n",
        "        # Create a graph of possible sentence transformations\n",
        "        graph = create_transformation_graph(text, t5_model, t5_tokenizer)\n",
        "        # Find the shortest path using Dijkstra's algorithm\n",
        "        cost, path = dijkstra(graph, text, graph[text][0][0])\n",
        "        corrected_sentence = path[-1] if path else text\n",
        "        detected_errors.append(error_type)\n",
        "        corrected_sentences.append(corrected_sentence)\n",
        "    return detected_errors, corrected_sentences\n",
        "\n",
        "# Take user input for testing the model\n",
        "while True:\n",
        "    user_input = input(\"Enter a sentence for error detection and correction (or type 'exit' to quit): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    detected_error_types, corrected_sentences = detect_and_correct_errors(user_input, best_rf_model, tfidf_vectorizer, kmeans, t5_model, t5_tokenizer, label_encoder)\n",
        "    for error_type, corrected_sentence in zip(detected_error_types, corrected_sentences):\n",
        "        print(f\"Detected Error Type: {error_type}\")\n",
        "        print(f\"Corrected Sentence: {corrected_sentence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACvWIxS3hOws",
        "outputId": "ff3760b7-15c6-430e-93f5-405c42debeb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column names in the dataset: Index(['Serial_Number', 'Error_Type', 'Ungrammatical_Statement',\n",
            "       'Standard_English'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Best parameters found:  {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': None}\n",
            "Random Forest Model evaluation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.22      0.33         9\n",
            "           1       0.57      0.36      0.44        11\n",
            "           2       0.10      0.09      0.10        11\n",
            "           3       0.17      0.23      0.19        22\n",
            "           4       0.17      0.17      0.17         6\n",
            "           5       0.33      0.44      0.38         9\n",
            "           6       0.44      0.40      0.42        10\n",
            "           7       0.10      0.14      0.12         7\n",
            "           8       0.00      0.00      0.00        12\n",
            "           9       0.62      0.67      0.64        12\n",
            "          10       0.17      0.08      0.11        12\n",
            "          11       0.25      0.07      0.11        15\n",
            "          12       0.00      0.00      0.00         9\n",
            "          13       0.80      0.89      0.84         9\n",
            "          14       0.33      0.20      0.25        10\n",
            "          15       0.77      1.00      0.87        10\n",
            "          16       0.33      0.25      0.29         4\n",
            "          17       0.33      0.17      0.22        12\n",
            "          18       0.28      0.47      0.35        15\n",
            "          19       0.00      0.00      0.00         4\n",
            "          20       0.18      0.20      0.19        10\n",
            "          21       0.40      0.48      0.44        25\n",
            "          22       0.22      0.20      0.21        10\n",
            "          23       0.15      0.22      0.18         9\n",
            "          24       0.62      1.00      0.77         5\n",
            "          25       0.00      0.00      0.00         1\n",
            "          26       0.25      0.11      0.15         9\n",
            "          27       0.00      0.00      0.00         5\n",
            "          28       0.11      0.11      0.11         9\n",
            "          29       0.29      0.25      0.27        24\n",
            "          30       0.33      0.14      0.20        14\n",
            "          31       0.12      0.21      0.15        19\n",
            "          32       0.30      0.44      0.36        18\n",
            "          33       0.50      0.11      0.18         9\n",
            "          34       0.17      0.21      0.19        19\n",
            "          35       0.50      0.11      0.18         9\n",
            "\n",
            "    accuracy                           0.28       404\n",
            "   macro avg       0.29      0.27      0.26       404\n",
            "weighted avg       0.30      0.28      0.27       404\n",
            "\n",
            "Accuracy: 0.27970297029702973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a sentence for error detection and correction (or type 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qBZVbrErl21G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import heapq\n",
        "from collections import defaultdict\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Uncomment these lines if you haven't downloaded the nltk data before\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('stopwords')\n",
        "#nltk.download('wordnet')\n",
        "\n",
        "# Load sample data\n",
        "data = pd.read_csv(\"/content/Grammar Correction.csv\")  # Replace with actual dataset path\n",
        "\n",
        "# Print column names to verify\n",
        "print(\"Column names in the dataset:\", data.columns)\n",
        "\n",
        "# Check if the necessary columns are present\n",
        "if 'Ungrammatical_Statement' not in data.columns or 'Error_Type' not in data.columns:\n",
        "    raise KeyError(\"The dataset must contain 'Ungrammatical_Statement' and 'Error_Type' columns\")\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(Ungrammatical_Statement):\n",
        "    # Lowercase conversion\n",
        "    # Ungrammatical_Statement = Ungrammatical_Statement.lower()\n",
        "    # Remove special characters\n",
        "    Ungrammatical_Statement = re.sub(r'\\W', ' ', Ungrammatical_Statement)\n",
        "    # Tokenization\n",
        "    words = word_tokenize(Ungrammatical_Statement)\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply preprocessing\n",
        "data['cleaned_text'] = data['Ungrammatical_Statement'].apply(preprocess_text)\n",
        "\n",
        "# Label encoding (assuming binary classification for errors)\n",
        "label_encoder = LabelEncoder()\n",
        "data['label'] = label_encoder.fit_transform(data['Error_Type'])\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(data['cleaned_text'])\n",
        "\n",
        "# Apply K-means clustering\n",
        "num_clusters = 10  # Set the number of clusters\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "data['cluster'] = kmeans.fit_predict(tfidf_matrix)\n",
        "\n",
        "# Add cluster labels as features\n",
        "X = np.hstack((tfidf_matrix.toarray(), data['cluster'].values.reshape(-1, 1)))\n",
        "y = data['label']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_grid, n_iter=10, cv=3, verbose=2, n_jobs=-1)\n",
        "\n",
        "# Fit RandomizedSearchCV to the training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best parameters found: \", random_search.best_params_)\n",
        "\n",
        "# Get the best model\n",
        "best_rf_model = random_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the test data\n",
        "y_pred_rf = best_rf_model.predict(X_test)\n",
        "print(\"Random Forest Model evaluation:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf)}\")\n",
        "\n",
        "# Load the pre-trained T5 model and tokenizer fine-tuned for grammar correction\n",
        "model_name = 'vennify/t5-base-grammar-correction'  # Known model for grammar correction\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Function to correct grammatical errors using the T5 model\n",
        "def correct_grammar(text, model, tokenizer):\n",
        "    input_text = \"gec: \" + text\n",
        "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(inputs, max_length=512, num_beams=4, early_stopping=True)\n",
        "    corrected_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return corrected_text\n",
        "\n",
        "# Dijkstra's algorithm for finding the shortest path in the graph of sentence transformations\n",
        "def dijkstra(graph, start, end):\n",
        "    queue, visited = [(0, start, [])], set()\n",
        "    heapq.heapify(queue)\n",
        "    while queue:\n",
        "        (cost, node, path) = heapq.heappop(queue)\n",
        "        if node in visited:\n",
        "            continue\n",
        "        path = path + [node]\n",
        "        if node == end:\n",
        "            return cost, path\n",
        "        visited.add(node)\n",
        "        for (next_node, c) in graph[node]:\n",
        "            if next_node not in visited:\n",
        "                heapq.heappush(queue, (cost + c, next_node, path))\n",
        "    return float(\"inf\"), []\n",
        "\n",
        "# Create a graph of possible sentence transformations\n",
        "def create_transformation_graph(sentence, model, tokenizer):\n",
        "    graph = defaultdict(list)\n",
        "    corrected_sentence = correct_grammar(sentence, model, tokenizer)\n",
        "    # For simplicity, assume each transformation has a cost of 1\n",
        "    graph[sentence].append((corrected_sentence, 1))\n",
        "    return graph\n",
        "\n",
        "# Function to detect errors and correct the sentence\n",
        "def detect_and_correct_errors(text, rf_model, vectorizer, kmeans, t5_model, t5_tokenizer, label_encoder):\n",
        "    # Preprocess the text for error detection\n",
        "    cleaned_text = preprocess_text(text)\n",
        "    text_tfidf = vectorizer.transform([cleaned_text])\n",
        "    text_cluster = kmeans.predict(text_tfidf)[0]\n",
        "    text_features = np.hstack((text_tfidf.toarray(), np.array([[text_cluster]])))\n",
        "    # Detect the error type\n",
        "    predicted_label = rf_model.predict(text_features)\n",
        "    # Check if the sentence has no errors based on some threshold\n",
        "    if predicted_label == 0:  # Assuming 0 is the label for \"no error\"\n",
        "        return \"No error in the sentence\", text\n",
        "    error_type = label_encoder.inverse_transform(predicted_label)[0]\n",
        "    # Create a graph of possible sentence transformations\n",
        "    graph = create_transformation_graph(text, t5_model, t5_tokenizer)\n",
        "    # Find the shortest path using Dijkstra's algorithm\n",
        "    cost, path = dijkstra(graph, text, graph[text][0][0])\n",
        "    corrected_sentence = path[-1] if path else text\n",
        "    return error_type, corrected_sentence\n",
        "\n",
        "# Take user input for testing the model\n",
        "while True:\n",
        "    user_input = input(\"Enter a sentence for error detection and correction (or type 'exit' to quit): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    detected_error_type, corrected_sentence = detect_and_correct_errors(user_input, best_rf_model, tfidf_vectorizer, kmeans, t5_model, t5_tokenizer, label_encoder)\n",
        "    print(f\"Detected Error Type: {detected_error_type}\")\n",
        "    print(f\"Corrected Sentence: {corrected_sentence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqZlblf-l23v",
        "outputId": "f03c05b9-5d74-4663-8fc0-2fa3dc3e7aab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column names in the dataset: Index(['Serial_Number', 'Error_Type', 'Ungrammatical_Statement',\n",
            "       'Standard_English'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found:  {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 30}\n",
            "Random Forest Model evaluation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.11      0.18         9\n",
            "           1       0.80      0.36      0.50        11\n",
            "           2       0.00      0.00      0.00        11\n",
            "           3       0.14      0.09      0.11        22\n",
            "           4       0.27      0.67      0.38         6\n",
            "           5       0.00      0.00      0.00         9\n",
            "           6       0.50      0.30      0.37        10\n",
            "           7       0.00      0.00      0.00         7\n",
            "           8       0.00      0.00      0.00        12\n",
            "           9       0.83      0.42      0.56        12\n",
            "          10       1.00      0.08      0.15        12\n",
            "          11       0.00      0.00      0.00        15\n",
            "          12       0.00      0.00      0.00         9\n",
            "          13       0.86      0.67      0.75         9\n",
            "          14       0.25      0.10      0.14        10\n",
            "          15       0.71      1.00      0.83        10\n",
            "          16       0.25      0.75      0.38         4\n",
            "          17       0.00      0.00      0.00        12\n",
            "          18       0.71      0.33      0.45        15\n",
            "          19       0.00      0.00      0.00         4\n",
            "          20       0.00      0.00      0.00        10\n",
            "          21       0.34      0.44      0.39        25\n",
            "          22       0.00      0.00      0.00        10\n",
            "          23       0.00      0.00      0.00         9\n",
            "          24       0.71      1.00      0.83         5\n",
            "          25       0.00      0.00      0.00         1\n",
            "          26       0.00      0.00      0.00         9\n",
            "          27       0.00      0.00      0.00         5\n",
            "          28       0.00      0.00      0.00         9\n",
            "          29       0.20      0.33      0.25        24\n",
            "          30       0.33      0.21      0.26        14\n",
            "          31       0.21      0.74      0.33        19\n",
            "          32       0.13      0.72      0.22        18\n",
            "          33       0.00      0.00      0.00         9\n",
            "          34       0.17      0.26      0.21        19\n",
            "          35       0.00      0.00      0.00         9\n",
            "\n",
            "    accuracy                           0.26       404\n",
            "   macro avg       0.25      0.24      0.20       404\n",
            "weighted avg       0.26      0.26      0.21       404\n",
            "\n",
            "Accuracy: 0.25742574257425743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a sentence for error detection and correction (or type 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OYH3DDQteF_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Uncomment these lines if you haven't downloaded the nltk data before\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load sample data\n",
        "data = pd.read_csv(\"/content/Grammar Correction.csv\")  # Replace with actual dataset path\n",
        "\n",
        "# Print column names to verify\n",
        "print(\"Column names in the dataset:\", data.columns)\n",
        "\n",
        "# Check if the necessary columns are present\n",
        "if 'Ungrammatical_Statement' not in data.columns or 'Error_Type' not in data.columns:\n",
        "    raise KeyError(\"The dataset must contain 'Ungrammatical_Statement' and 'Error_Type' columns\")\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(Ungrammatical_Statement):\n",
        "    # Lowercase conversion\n",
        "    # Ungrammatical_Statement = Ungrammatical_Statement.lower()\n",
        "    # Remove special characters\n",
        "    Ungrammatical_Statement = re.sub(r'\\W', ' ', Ungrammatical_Statement)\n",
        "    # Tokenization\n",
        "    words = word_tokenize(Ungrammatical_Statement)\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply preprocessing\n",
        "data['cleaned_text'] = data['Ungrammatical_Statement'].apply(preprocess_text)\n",
        "\n",
        "# Label encoding (assuming binary classification for errors)\n",
        "label_encoder = LabelEncoder()\n",
        "data['label'] = label_encoder.fit_transform(data['Error_Type'])\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(data['cleaned_text'])\n",
        "\n",
        "# Apply K-means clustering\n",
        "num_clusters = 10  # Set the number of clusters\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "data['cluster'] = kmeans.fit_predict(tfidf_matrix)\n",
        "\n",
        "# Add cluster labels as features\n",
        "X = np.hstack((tfidf_matrix.toarray(), data['cluster'].values.reshape(-1, 1)))\n",
        "y = data['label']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_grid, n_iter=10, cv=3, verbose=2, n_jobs=-1)\n",
        "\n",
        "# Fit RandomizedSearchCV to the training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best parameters found: \", random_search.best_params_)\n",
        "\n",
        "# Get the best model\n",
        "best_rf_model = random_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the test data\n",
        "y_pred_rf = best_rf_model.predict(X_test)\n",
        "print(\"Random Forest Model evaluation:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf)}\")\n",
        "\n",
        "# Load the pre-trained T5 model and tokenizer fine-tuned for grammar correction\n",
        "model_name = 'vennify/t5-base-grammar-correction'  # Known model for grammar correction\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Function to correct grammatical errors using the T5 model\n",
        "def correct_grammar(text, model, tokenizer):\n",
        "    input_text = \"gec: \" + text\n",
        "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(inputs, max_length=512, num_beams=4, early_stopping=True)\n",
        "    corrected_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return corrected_text\n",
        "\n",
        "# Function to detect errors and correct the sentence\n",
        "def detect_and_correct_errors(text, rf_model, vectorizer, kmeans, t5_model, t5_tokenizer, label_encoder):\n",
        "    # Preprocess the text for error detection\n",
        "    cleaned_text = preprocess_text(text)\n",
        "    text_tfidf = vectorizer.transform([cleaned_text])\n",
        "    text_cluster = kmeans.predict(text_tfidf)[0]\n",
        "    text_features = np.hstack((text_tfidf.toarray(), np.array([[text_cluster]])))\n",
        "    # Detect the error type\n",
        "    predicted_label = rf_model.predict(text_features)\n",
        "    # Check if the sentence has no errors based on some threshold\n",
        "    if predicted_label == 0:  # Assuming 0 is the label for \"no error\"\n",
        "        return \"No error in the sentence\", text\n",
        "    error_type = label_encoder.inverse_transform(predicted_label)[0]\n",
        "    # Correct the sentence using the T5 model\n",
        "    corrected_sentence = correct_grammar(text, t5_model, t5_tokenizer)\n",
        "    return error_type, corrected_sentence\n",
        "\n",
        "# Take user input for testing the model\n",
        "while True:\n",
        "    user_input = input(\"Enter a sentence for error detection and correction (or type 'exit' to quit): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    detected_error_type, corrected_sentence = detect_and_correct_errors(user_input, best_rf_model, tfidf_vectorizer, kmeans, t5_model, t5_tokenizer, label_encoder)\n",
        "    print(f\"Detected Error Type: {detected_error_type}\")\n",
        "    print(f\"Corrected Sentence: {corrected_sentence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9l4hQx-eHLM",
        "outputId": "87401c1a-d817-4622-d5d9-ef35fdb1b202"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column names in the dataset: Index(['Serial_Number', 'Error_Type', 'Ungrammatical_Statement',\n",
            "       'Standard_English'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found:  {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n",
            "Random Forest Model evaluation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.11      0.15         9\n",
            "           1       0.67      0.36      0.47        11\n",
            "           2       0.00      0.00      0.00        11\n",
            "           3       0.14      0.14      0.14        22\n",
            "           4       0.18      0.50      0.26         6\n",
            "           5       0.36      0.44      0.40         9\n",
            "           6       0.60      0.30      0.40        10\n",
            "           7       0.33      0.14      0.20         7\n",
            "           8       0.00      0.00      0.00        12\n",
            "           9       0.67      0.50      0.57        12\n",
            "          10       0.14      0.08      0.11        12\n",
            "          11       0.00      0.00      0.00        15\n",
            "          12       0.00      0.00      0.00         9\n",
            "          13       0.75      0.67      0.71         9\n",
            "          14       0.50      0.30      0.37        10\n",
            "          15       0.83      1.00      0.91        10\n",
            "          16       0.50      0.50      0.50         4\n",
            "          17       0.00      0.00      0.00        12\n",
            "          18       0.33      0.33      0.33        15\n",
            "          19       0.00      0.00      0.00         4\n",
            "          20       0.18      0.20      0.19        10\n",
            "          21       0.32      0.44      0.37        25\n",
            "          22       0.00      0.00      0.00        10\n",
            "          23       0.11      0.22      0.14         9\n",
            "          24       0.71      1.00      0.83         5\n",
            "          25       0.00      0.00      0.00         1\n",
            "          26       0.60      0.33      0.43         9\n",
            "          27       0.00      0.00      0.00         5\n",
            "          28       0.20      0.11      0.14         9\n",
            "          29       0.30      0.33      0.31        24\n",
            "          30       0.29      0.14      0.19        14\n",
            "          31       0.22      0.63      0.32        19\n",
            "          32       0.21      0.56      0.30        18\n",
            "          33       0.50      0.11      0.18         9\n",
            "          34       0.33      0.47      0.39        19\n",
            "          35       0.50      0.11      0.18         9\n",
            "\n",
            "    accuracy                           0.29       404\n",
            "   macro avg       0.30      0.28      0.26       404\n",
            "weighted avg       0.29      0.29      0.27       404\n",
            "\n",
            "Accuracy: 0.29455445544554454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a sentence for error detection and correction (or type 'exit' to quit): i goes to the store everyday.\n",
            "Detected Error Type: Verb Tense Errors\n",
            "Corrected Sentence: I go to the store every day.\n",
            "Enter a sentence for error detection and correction (or type 'exit' to quit): I wanting a book.\n",
            "Detected Error Type: Spelling Mistakes\n",
            "Corrected Sentence: I want a book.\n",
            "Enter a sentence for error detection and correction (or type 'exit' to quit): they eating a pizza for lunch.\n",
            "Detected Error Type: Verb Tense Errors\n",
            "Corrected Sentence: They are eating a pizza for lunch.\n",
            "Enter a sentence for error detection and correction (or type 'exit' to quit): He is an inteeligent boy.\n",
            "Detected Error Type: Negation Errors\n",
            "Corrected Sentence: He is an intelligent boy.\n",
            "Enter a sentence for error detection and correction (or type 'exit' to quit): He is an intelligent boy.\n",
            "Detected Error Type: Negation Errors\n",
            "Corrected Sentence: He is an intelligent boy.\n",
            "Enter a sentence for error detection and correction (or type 'exit' to quit): i is a smart boy.\n",
            "Detected Error Type: Capitalization Errors\n",
            "Corrected Sentence: I am a smart boy.\n",
            "Enter a sentence for error detection and correction (or type 'exit' to quit): i was completed the project.\n",
            "Detected Error Type: Capitalization Errors\n",
            "Corrected Sentence: I completed the project.\n",
            "Enter a sentence for error detection and correction (or type 'exit' to quit): exit\n"
          ]
        }
      ]
    }
  ]
}